{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3594108,
          "sourceType": "datasetVersion",
          "datasetId": 2156255
        }
      ],
      "dockerImageVersionId": 30213,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": " Credit Card Fraud",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Preetha06/Major-task2/blob/main/Credit_Card_Fraud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'credit-card-fraud:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2156255%2F3594108%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240210%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240210T080338Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da90a4ddeea2a3b9f856237ba45f20cb3ec5f50932a40fc319095aa1006ecb27fa6ebf61f6c48bbfcd476f7717506a1c96f6d48c3b3ecfb87c0f1cc625b5cc3b177f7b640ed6e12f3697a552800f843382dcb23fa97208f3dd8a277f65e83c49c0d5512a32b7b6befa85e3a1ce46452e8db20acb1c1c01bcf222d30e23917c508dd86440135bf72978e4f63f40945c04f2ee08855ca28b192d1ee4a15ecae189e25528e137afb97b906201fdf49ceff42d59aab88afae18b6ad1e29fce0f7111c20c1322ffada6195acfbd8630ad9e77ad61011cd9c3ae1d1cd19583ed9bd310d01f90de240ea7b568e7b032ad8ad523081a1ae8d4886beaa5cebcc4db827dc77'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "REuPHPsdDhnE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Hello this is part of an article on Anomoly Detection. If you are interested in learning more check it out at the link below.\n",
        "\n",
        "https://medium.com/@bdatamethods/data-mining-anomaly-detection-with-python-1d088d93da80"
      ],
      "metadata": {
        "id": "msYm-OeKDhnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Process\n",
        "\n",
        "How are credit cards used and what can we understand about their use to identify credit card fraud?\n",
        "\n",
        "There are many ways to use credit cards. Some people use them for everyday purchases, such as gas or groceries. Other people use them for larger purchases, such as a new television or a vacation. And still others use them for emergencies, such as when their car breaks down or they have unexpected medical bills. Whatever the occasion people tend to follow certain trends in their spending.\n",
        "\n",
        "Aside from trends in purchases there are also trends in where and how the cards are used. A person could regularly use their card in a specific city one day or they only use the chip on the card instead of using it for online purchases.  All of these trends can be monitored to help identify potential fraud.\n",
        "\n",
        "If there are suddenly large purchases being made on a credit card that is usually only used for small transactions, that could be a sign of fraud. Or if the card is being used in a city where the person doesn't live or work, that could also be suspicious."
      ],
      "metadata": {
        "id": "WNdNyNthDhnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries for this project\n",
        "\n",
        "I like to dedicate a block to the libraries used in project. This will be the first step."
      ],
      "metadata": {
        "id": "TyU4qOAeDhnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# linear algebra\n",
        "import numpy as np\n",
        "\n",
        "# data processing library\n",
        "import pandas as pd\n",
        "\n",
        "# pyplot from matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# high level visualization package\n",
        "import seaborn as sns\n",
        "\n",
        "# import library to generate training and testing datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import library to help scale features in the datsets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# import encoding library to transform data values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# import K Nearest Neighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# import Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# import confusion matrix with Yellowbrick\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from yellowbrick.classifier import ClassificationReport\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:03.128023Z",
          "iopub.execute_input": "2024-01-02T05:07:03.129163Z",
          "iopub.status.idle": "2024-01-02T05:07:04.785051Z",
          "shell.execute_reply.started": "2024-01-02T05:07:03.129055Z",
          "shell.execute_reply": "2024-01-02T05:07:04.783927Z"
        },
        "trusted": true,
        "id": "ij7GsNQuDhnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Data\n",
        "Now that we have an understanding of some nuances to credit card usage lets take a look at the data.\n",
        "The next step is loading the data into a dataframe and viewing the raw data. Our data is stored in a csv so all we have to do is read the csv into a dataframe."
      ],
      "metadata": {
        "id": "gM7_pQjeDhnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read csv file to dataframe\n",
        "df = pd.read_csv(r'../input/credit-card-fraud/card_transdata.csv')\n",
        "\n",
        "# display dataframe\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:04.786675Z",
          "iopub.execute_input": "2024-01-02T05:07:04.787547Z",
          "iopub.status.idle": "2024-01-02T05:07:07.531178Z",
          "shell.execute_reply.started": "2024-01-02T05:07:04.787486Z",
          "shell.execute_reply": "2024-01-02T05:07:07.530048Z"
        },
        "trusted": true,
        "id": "z4nvljxUDhnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 1 million entries and 8 columns. The 8 columns are:\n",
        "\n",
        "* distance_from_home - Distance from home where transaction occured\n",
        "* distance_from_last_transaction - Distance from where last transaction occured\n",
        "* ratio_to_median_purchase_price - Ratio of purchased price transaction to median purchase price\n",
        "* repeat_retailer - Has historically purchased from retailer (1 = Yes / 0 = No)\n",
        "* used_chip - Was chip used in transaction (1 = Yes / 0 = No)\n",
        "* used_pin_number - Was PIN used to complete transaction (1 = Yes / 0 = No)\n",
        "* online_order - Was transaction an online order (1 = Yes / 0 = No)\n",
        "* fraud - Was transaction fraudulent (1 = Yes / 0 = No)\n",
        "\n",
        "Based on how the data has been organized we can assume it doesn't represent a single client. It is a collection of transactions from many clients derived from a combination of data sources.\n",
        "\n",
        "Variables relating to distance have no unit of measure attached to them but, we can assume this data uses the same unit of measure. We care more about relations between distance and other variables.\n",
        "\n",
        "The data type of all variabes are float. I prefer the binary variables be integer values but, this wont change the outcome so we will let them be.\n",
        "\n",
        "Let's take a closer look at each variable."
      ],
      "metadata": {
        "id": "RIWYELg-DhnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:07.533033Z",
          "iopub.execute_input": "2024-01-02T05:07:07.5334Z",
          "iopub.status.idle": "2024-01-02T05:07:08.006855Z",
          "shell.execute_reply.started": "2024-01-02T05:07:07.533356Z",
          "shell.execute_reply": "2024-01-02T05:07:08.005873Z"
        },
        "trusted": true,
        "id": "m71cUrA-DhnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The count of each column is exactly 1 million which is a great indicator that we don't have any missing values.\n",
        "\n",
        "Approximately 88% of transactions were with a repeat retailer. 35% used a chip. 10% used a pin. 65 % were online orders, that matches with the chip transactions.\n",
        "\n",
        "Only 8.7% of transactions were fraudulant.\n",
        "\n",
        "There's a pretty large gap between the minimum distance and max distance. The next step is grouping the distance and ratio values and generating a group so we can compare the values to the number of fraudulant cases.\n",
        "\n",
        "I used the percentage values from the describe() function to determine the grouping for each variable."
      ],
      "metadata": {
        "id": "iBs1d5s2DhnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create temporary dataframe\n",
        "df_temp = df\n",
        "\n",
        "# how distance from home groups are organized\n",
        "dfh_groups = [0, 10, 25, 100, 1000, 5000, 11000]\n",
        "\n",
        "# how distance from last transaction groups are organized\n",
        "dflt_groups = [0, 1, 2, 3, 100, 1000, 5000, 12000]\n",
        "\n",
        "# how ratio to median purchase groups are organized\n",
        "rtmpp_groups = [0, .5, 1, 1.5, 2, 10, 50, 100, 200, 250, 300]\n",
        "\n",
        "# split distance into groups\n",
        "df_temp['dist_home_group'] = pd.cut(df['distance_from_home'], dfh_groups)\n",
        "df_temp['dist_trans_group'] = pd.cut(df['distance_from_last_transaction'], dflt_groups)\n",
        "df_temp['rtmpp_group'] = pd.cut(df['ratio_to_median_purchase_price'], rtmpp_groups)\n",
        "\n",
        "# check number of entries with distances\n",
        "print(\"Distance from Home \")\n",
        "print(df_temp['dist_home_group'].value_counts().sort_index())\n",
        "print(\"\\n\")\n",
        "print(\"Distance from last transaction\")\n",
        "print(df_temp['dist_trans_group'].value_counts().sort_index())\n",
        "print(\"\\n\")\n",
        "print(\"Ratio to median purchase price\")\n",
        "print(df_temp['rtmpp_group'].value_counts().sort_index())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:08.009149Z",
          "iopub.execute_input": "2024-01-02T05:07:08.010203Z",
          "iopub.status.idle": "2024-01-02T05:07:08.179929Z",
          "shell.execute_reply.started": "2024-01-02T05:07:08.010158Z",
          "shell.execute_reply": "2024-01-02T05:07:08.178712Z"
        },
        "trusted": true,
        "id": "91EQHX3KDhnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count number fraudulant transactions in distance group.\n",
        "df_temp.groupby(['dist_home_group'])['fraud'].apply(lambda fraud: (fraud==1).sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:08.18384Z",
          "iopub.execute_input": "2024-01-02T05:07:08.184198Z",
          "iopub.status.idle": "2024-01-02T05:07:08.253551Z",
          "shell.execute_reply.started": "2024-01-02T05:07:08.184164Z",
          "shell.execute_reply": "2024-01-02T05:07:08.252409Z"
        },
        "trusted": true,
        "id": "y3cYAdlWDhnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the count of fraudulant transactions for columns 'dis_home_group',\n",
        "cols = [ 'dist_home_group', 'dist_trans_group', 'rtmpp_group', 'repeat_retailer',\n",
        "        'used_pin_number', 'used_chip', 'online_order']\n",
        "\n",
        "n_rows = 3\n",
        "n_cols = 3\n",
        "\n",
        "# The subplot grid and the figure size of each graph\n",
        "# This returns a Figure (fig) and an Axes Object (axs)\n",
        "fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*6,n_rows*4))\n",
        "\n",
        "# Iterates through the matrix and places a graph\n",
        "for r in range(0,n_rows):\n",
        "    for c in range(0,n_cols):\n",
        "        # prevents out of range error and hides graph template\n",
        "        if r == 2 and c > 0:\n",
        "            fig.delaxes(axs[r][c])\n",
        "            fig.delaxes(axs[r][c+1])\n",
        "            break\n",
        "        i = r*n_cols+ c #index to go through the number of columns\n",
        "        ax = axs[r][c] #Show where to position each subplot\n",
        "        sns.countplot(x=df_temp[cols[i]], hue=df_temp[\"fraud\"], ax=ax)\n",
        "        ax.legend(title=\"fraud\", loc='upper right')\n",
        "\n",
        "plt.tight_layout()   #tight_layout"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:08.256401Z",
          "iopub.execute_input": "2024-01-02T05:07:08.257712Z",
          "iopub.status.idle": "2024-01-02T05:07:11.062575Z",
          "shell.execute_reply.started": "2024-01-02T05:07:08.257674Z",
          "shell.execute_reply": "2024-01-02T05:07:11.061742Z"
        },
        "trusted": true,
        "id": "kpz6BKnyDhnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on these graphs, there is a relation to high dollar purchases and fraudulant transactions. This makes sense. A scammer would want to extract as much money as they could before the card is blocked.\n",
        "\n",
        "Fraud and repeated retailers suggests scammers using a retailer they are familiar with. If making purchases in person they know the stock or they are familiar with with the online purchasing process.\n",
        "\n",
        "It is no surprise fraudulant transactions occur without the chip or pin. Online orders allow more anonymous purchases and are quicker to process.\n",
        "\n",
        "Next let's observe any correlation between variables. Before doing that we need to convert the distance and ratio groups to numerical values in order for the correlation matrix to process them."
      ],
      "metadata": {
        "id": "qFeIhdgZDhnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp.dtypes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:11.06404Z",
          "iopub.execute_input": "2024-01-02T05:07:11.064653Z",
          "iopub.status.idle": "2024-01-02T05:07:11.074017Z",
          "shell.execute_reply.started": "2024-01-02T05:07:11.064615Z",
          "shell.execute_reply": "2024-01-02T05:07:11.072659Z"
        },
        "trusted": true,
        "id": "GNASbtpZDhnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start transforming data into numerical values\n",
        "labelencoder = LabelEncoder()\n",
        "\n",
        "# encode dist_home_group column\n",
        "df_temp.iloc[:,8] = labelencoder.fit_transform(df.iloc[:,8].values)\n",
        "\n",
        "# encode dist_trans_group column\n",
        "df_temp.iloc[:,9] = labelencoder.fit_transform(df.iloc[:,9].values)\n",
        "\n",
        "# encode rtmpp_group column\n",
        "df_temp.iloc[:,10] = labelencoder.fit_transform(df.iloc[:,10].values)\n",
        "\n",
        "df_temp"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:11.075673Z",
          "iopub.execute_input": "2024-01-02T05:07:11.076073Z",
          "iopub.status.idle": "2024-01-02T05:07:24.1893Z",
          "shell.execute_reply.started": "2024-01-02T05:07:11.07604Z",
          "shell.execute_reply": "2024-01-02T05:07:24.187759Z"
        },
        "trusted": true,
        "id": "I3t_xXQVDhnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate correlation matrix\n",
        "corrmat = df_temp.corr()\n",
        "\n",
        "# setup figure size\n",
        "fig = plt.figure(figsize = (12, 9))\n",
        "\n",
        "# assign data to a heatmap\n",
        "sns.heatmap(corrmat, square = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:24.190907Z",
          "iopub.execute_input": "2024-01-02T05:07:24.191286Z",
          "iopub.status.idle": "2024-01-02T05:07:25.099842Z",
          "shell.execute_reply.started": "2024-01-02T05:07:24.191251Z",
          "shell.execute_reply": "2024-01-02T05:07:25.098435Z"
        },
        "trusted": true,
        "id": "WmCEp4bDDhnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is very little correlation between each variable. The only thing that stands out is repeat_retailer and distance_from_home."
      ],
      "metadata": {
        "id": "5pp6eb8RDhnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Data for Model Training\n",
        "\n",
        "Frist we must split the data into a training and testing datasets. For this we use the 80/20 rule. 80% of the data is for training and 20% is for testing."
      ],
      "metadata": {
        "id": "C08MaIjPDhnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZyXmLZIoDhnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into independent 'X' variables or features\n",
        "X = df.iloc[:, 0:6].values\n",
        "\n",
        "# create a target dataset that can be used for training and validation\n",
        "Y = df.replace({'fraud': {1:'fraud', 0: 'not_fraud'}})['fraud']\n",
        "\n",
        "# split dataset into 80% training set and 20% testing set\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "print(\"completed\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:25.103405Z",
          "iopub.execute_input": "2024-01-02T05:07:25.103785Z",
          "iopub.status.idle": "2024-01-02T05:07:25.673222Z",
          "shell.execute_reply.started": "2024-01-02T05:07:25.103735Z",
          "shell.execute_reply": "2024-01-02T05:07:25.671937Z"
        },
        "trusted": true,
        "id": "KfHoHyuMDhnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation with K Nearest Neighbors\n",
        "\n",
        "First we will evaluate the data using K Nearest Neighbors then we will run an evaluation using Logistic Regression and compare the results."
      ],
      "metadata": {
        "id": "F3OY7ebnDhnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the classification model\n",
        "model = KNeighborsClassifier()\n",
        "\n",
        "# The confusion Matrix visualizer takes a model\n",
        "cm = ConfusionMatrix(model, percent=True)\n",
        "\n",
        "# fit fits the passed model. This is unnecessary if you\n",
        "# pass the visualize a pre-fitted model\n",
        "cm.fit(X_train, Y_train)\n",
        "\n",
        "# To create the confusion matrix, we need some test data.\n",
        "# Score runs predict() on the data dn then creates the\n",
        "# confusion matrix from scikit learn.\n",
        "cm.score(X_test, Y_test)\n",
        "\n",
        "# change fontsize of the labels in the figure\n",
        "for label in cm.ax.texts:\n",
        "    label.set_size(20)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:07:25.674509Z",
          "iopub.execute_input": "2024-01-02T05:07:25.674825Z",
          "iopub.status.idle": "2024-01-02T05:08:18.292506Z",
          "shell.execute_reply.started": "2024-01-02T05:07:25.674796Z",
          "shell.execute_reply": "2024-01-02T05:08:18.290729Z"
        },
        "trusted": true,
        "id": "GJL6Ohg4DhnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is used to describe the performance of a classification model. From this we gather that 80% were correctly predicted not fraud and 97% were correctly predicted to be fraud. The other two values indicate incorrect predictions."
      ],
      "metadata": {
        "id": "awcZiyUBDhnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN - Precision, Recall and F1 Score\n"
      ],
      "metadata": {
        "id": "au06PAM_DhnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the size of the figure and the font size\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15,7)\n",
        "plt.rcParams['font.size'] = 20\n",
        "\n",
        "# Instantiate the visualizer\n",
        "visualizer = ClassificationReport(model)\n",
        "\n",
        "# Fit the training data to the visualizer\n",
        "visualizer.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "visualizer.score(X_test, Y_test)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:08:18.2949Z",
          "iopub.execute_input": "2024-01-02T05:08:18.295825Z",
          "iopub.status.idle": "2024-01-02T05:09:09.798721Z",
          "shell.execute_reply.started": "2024-01-02T05:08:18.295757Z",
          "shell.execute_reply": "2024-01-02T05:09:09.797425Z"
        },
        "trusted": true,
        "id": "IiwpG2nADhnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation with Logistic Regression"
      ],
      "metadata": {
        "id": "rt6n_pk8DhnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the classification model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# The confusion Matrix visualizer takes a model\n",
        "cm = ConfusionMatrix(model, percent=True)\n",
        "\n",
        "# fit fits the passed model. This is unnecessary if you\n",
        "# pass the visualize a pre-fitted model\n",
        "cm.fit(X_train, Y_train)\n",
        "\n",
        "# To create the confusion matrix, we need some test data.\n",
        "# Score runs predict() on the data dn then creates the\n",
        "# confusion matrix from scikit learn.\n",
        "cm.score(X_test, Y_test)\n",
        "\n",
        "# change fontsize of the labels in the figure\n",
        "for label in cm.ax.texts:\n",
        "    label.set_size(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:09:09.799989Z",
          "iopub.execute_input": "2024-01-02T05:09:09.800327Z",
          "iopub.status.idle": "2024-01-02T05:09:18.62439Z",
          "shell.execute_reply.started": "2024-01-02T05:09:09.800296Z",
          "shell.execute_reply": "2024-01-02T05:09:18.622399Z"
        },
        "trusted": true,
        "id": "ny7MfBr_DhnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression - Precision, Recall and F1 Score"
      ],
      "metadata": {
        "id": "WonF6eT-DhnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the size of the figure and the font size\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15,7)\n",
        "plt.rcParams['font.size'] = 20\n",
        "\n",
        "# Instantiate the visualizer\n",
        "visualizer = ClassificationReport(model)\n",
        "\n",
        "# Fit the training data to the visualizer\n",
        "visualizer.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "visualizer.score(X_test, Y_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-02T05:09:18.627219Z",
          "iopub.execute_input": "2024-01-02T05:09:18.627861Z",
          "iopub.status.idle": "2024-01-02T05:09:24.677823Z",
          "shell.execute_reply.started": "2024-01-02T05:09:18.627804Z",
          "shell.execute_reply": "2024-01-02T05:09:24.676617Z"
        },
        "trusted": true,
        "id": "KyRwWc_ODhnP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}